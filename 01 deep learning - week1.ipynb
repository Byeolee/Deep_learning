{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T05:42:37.168200Z",
     "start_time": "2018-01-11T05:42:37.161496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(){function insertScript(source){var s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src',source);s.setAttribute('class','jupyter-sidenote-script');if(typeof s!='undefined'){document.getElementsByTagName('head')[0].appendChild(s)}}var src='https://jangxyz.github.io/jupyter-sidenote/dist/main.min.js';insertScript(src+'?'+Date.now());})()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "(function(){function insertScript(source){var s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src',source);s.setAttribute('class','jupyter-sidenote-script');if(typeof s!='undefined'){document.getElementsByTagName('head')[0].appendChild(s)}}var src='https://jangxyz.github.io/jupyter-sidenote/dist/main.min.js';insertScript(src+'?'+Date.now());})()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 딥러닝반 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "이번 DS School 딥러닝반에서는 딥러닝 알고리즘의 원리를 하나하나 이해하면서 딥러닝이 구체적으로 어떻게 동작하는지를 배웁니다.\n",
    "\n",
    "이 방식은 [DS School 입문과정](https://www.dsschool.co.kr/suggestions#headingOne)과 많이 다른데, 입문반에서는 이미 만들어진 알고리즘(Decision Tree, Random Forest, etc)을 잘 활용하고, 이 알고리즘에 좋은 Feature를 집어넣는 방법에 대해서 집중적으로 다뤘습니다.\n",
    "\n",
    "하지만 딥러닝은 이런 방식을 사용할 수도, 사용하기에 적합하지도 않습니다. 먼저 딥러닝이라는 알고리즘의 최대 장점은 **\"Feature를 스스로 찾아서 분석한다.\"**라는 것에 있습니다. 그렇기 때문에 딥러닝이 아닌 다른 알고리즘을 사용할 때 했던 Feature Engineering(새로운 Feature를 찾아내는 작업)이나 Feature Selection(Feature를 골라내는 작업)의 비중이 상대적으로 낮습니다. 반면 딥러닝은 다른 알고리즘에 비해 튜닝해야 하는 설정들이 매우 복잡하며, 이 설정들을 제대로 이해하지 못하면 **딥러닝 알고리즘이 제대로 동작하지 않습니다.** 제대로 동작하지 않는다는 것의 의미는 튜닝하기 전보다 성능이 낮아진다는게 아니라, 말 그대로 딥러닝 알고리즘이 제대로 동작하지 않는다는 의미입니다.\n",
    "\n",
    "그렇기 때문에 딥러닝반에서는, 딥러닝 알고리즘의 원리를 하나하나 파악해가는 방식으로 수업을 진행할 것입니다.\n",
    "\n",
    "\n",
    "이번 딥러닝반을 수강하는 데 필요한 사전 준비는 1) 약간의 파이썬 지식, 그리고 2) 약간의 수학입니다. \n",
    "\n",
    "* **파이썬**: 파이썬의 모든 기능을 다 알 필요는 없고, 함수와 클래스의 원리와 사용 방법 정도만 숙지하고 있으면 충분합니다. [점프 투 파이썬](https://wikidocs.net/book/1) 책을 읽으면 큰 도움이 될 겁니다.\n",
    "* **수학**: 고등학교 수준의 미분을 다룰 수 있으면 충분하며, 편미분을 이해할 수 있다면 플러스입니다. 미분 문제를 잘 푸는 것이 아닌, 미분/편미분의 기본적인 원리는 이해하는 것 만으로 충분합니다. [수포자도 쉽게 알 수 있는 수학/미분과 적분](https://librewiki.net/wiki/%EC%8B%9C%EB%A6%AC%EC%A6%88:%EC%88%98%ED%8F%AC%EC%9E%90%EB%8F%84_%EC%89%BD%EA%B2%8C_%EC%95%8C_%EC%88%98_%EC%9E%88%EB%8A%94_%EC%88%98%ED%95%99/%EB%AF%B8%EB%B6%84%EA%B3%BC_%EC%A0%81%EB%B6%84) 을 읽고 이해할 수 있다면 수업을 따라오는데 큰 지장은 없습니다.\n",
    "\n",
    "\n",
    "현재 딥러닝이 가장 많이 사용되고 있는 분야는 1) 이미지, 2) 자연어, 3) 음성인식입니다. 본 과정에서는 이 중 이미지, 그것도 이미지 분류(Image Classification)에 집중합니다. 이미지 분류를 이해할 수 있다면 나머지 분야는 어렵지 않기 때문에 독학으로도 충분히 따라올 수 있을 겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:43:45.565005Z",
     "start_time": "2018-01-08T14:43:45.404295Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "numpy는 선형대수 라이브러리입니다. 빠른 수학 연산을 수행하는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:43:49.244045Z",
     "start_time": "2018-01-08T14:43:49.227966Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] + [4, 5, 6]\n",
    "\n",
    "np.array([1, 2, 3]) + np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:43:44.190281Z",
     "start_time": "2018-01-08T14:43:44.176853Z"
    },
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "일반적으로 파이썬의 리스트 두 개를 서로 더하면 이어붙이기(concatenation)가 됩니다.\n",
    "\n",
    "같은 크기의 `np.array` 두 개를  서로 더하면 각각 더한 합이 나타납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:43:59.572442Z",
     "start_time": "2018-01-08T14:43:59.563383Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] * 2\n",
    "\n",
    "np.array([1, 2, 3]) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "파이썬의 리스트에 숫자를 곱하면 하면 그 곱만큼 리스트 원소가 반복됩니다.\n",
    "\n",
    "`np.array`에 숫자를 곱하면 각각의 원소에 그 값을 곱한 결과가 나타납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:16:41.388735Z",
     "start_time": "2018-01-08T15:16:41.305975Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 시각화 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# jupyter 화면에 그래프를 그대로 보여주도록 설정하는 옵션\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:45:49.084334Z",
     "start_time": "2018-01-08T14:45:49.078345Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "시각화 라이브러리로 유명한 `matplotlib`을 사용합니다.\n",
    "\n",
    "데이터를 가지고 그린 그래프를 jupyter 화면에 바로 보여주기 위해, `%matplotlib inline` 명령을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:45:58.594257Z",
     "start_time": "2018-01-08T14:45:58.588345Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3172644129064527"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "`np.random.uniform()` 함수는 0과 1 사이에서 임의의 값을 균등하게 가져옵니다. `0.001`을 가져올 확률과 `0.5`를 가져올 확률이 모두 동일하다는 뜻입니다.\n",
    "\n",
    "(다른 함수 중에는 균등하게가 아니라 중앙값에 가까울 수록 더 자주 가져오도록 정규분포를 따르는 함수가 있습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:48:34.506825Z",
     "start_time": "2018-01-08T14:48:34.498099Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.56903808,  0.15934522,  0.56968215,  0.34992133,  0.43224278,\n",
       "        0.87880562,  0.94300368,  0.20485124,  0.56498275,  0.25234819])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x.shape)\n",
    "x[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:49:08.697874Z",
     "start_time": "2018-01-08T14:49:08.692730Z"
    },
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "`size`를 지정하면 랜덤값을 한번에 여러개 가져올 수 있습니다. 0과 1 사이의 값을 100개 가져오고 있습니다.\n",
    "\n",
    "`.shape`을 통해 `array`의 크기가 100이라는 것을 확인할 수 있습니다. <br>\n",
    "데이터가 어떻게 생겼는지 앞의 10개만 살펴보고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:52:18.105602Z",
     "start_time": "2018-01-08T14:52:18.098235Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "x = [ 0.56903808  0.15934522  0.56968215  0.34992133  0.43224278  0.87880562]\n",
      "y = [ 0.17071142  0.04780357  0.17090464  0.1049764   0.12967283  0.26364169]\n"
     ]
    }
   ],
   "source": [
    "# x의 모든 값에 0.3을 곱한다.\n",
    "y = 0.3 * x\n",
    "\n",
    "print(y.shape)\n",
    "print('x =', x[0:6])\n",
    "print('y =', y[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "`np.array`에 숫자를 곱하면 각각의 원소에 숫자만큼 곱해지게 됩니다. 즉 `y` 배열(array)은 크기가 100인 `x` 배열의 모든 원소에 0.3을 곱한 원소들이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "x와 y의 값을 시각화 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:52:53.129472Z",
     "start_time": "2018-01-08T14:52:52.696402Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x114c2e860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+mZmKfL/AcCxCugkviPUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c6b22e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:53:42.636870Z",
     "start_time": "2018-01-08T14:53:42.631549Z"
    },
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "`matplotlib`의 `scatter` 함수를 이용해서 x와 y를 2차원상에 점으로 시각화 해보았습니다.\n",
    "\n",
    "모두 일관되게 한 직선 위에 나타나는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 학습 목표 - 데이터만으로 기울기를 찾아내자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:54:07.083021Z",
     "start_time": "2018-01-08T14:54:07.077760Z"
    },
    "hidden": true
   },
   "source": [
    "** 목표 **\n",
    "\n",
    "이 x, y 데이터를 이용해서 학습을 하도록 하겠습니다.\n",
    "\n",
    "우리의 알고리즘은 x와 y의 관계를 모른채, 주어진 데이터만 보고 그 기울기를 학습해서 찾아낼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Case 1 - Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:57:58.560780Z",
     "start_time": "2018-01-08T14:57:58.555189Z"
    },
    "hidden": true
   },
   "source": [
    "첫 시도로는 무작위로 x와 y의 관계를 추측하는 방식을 사용해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**전략**: $x$에 랜덤한 값을 곱해 $y_{predict}$를 만든다. 이 추정값 $y_{predict}$가 실제값 $y$와 차이가 적으면 적을수록 바람직하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T14:59:14.281281Z",
     "start_time": "2018-01-08T14:59:14.218632Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4594422967439107 0.0722792054296\n",
      "0.4052803998029687 0.047726254579\n",
      "0.5182282212011502 0.0989283443154\n",
      "0.7328035852860489 0.196200756577\n",
      "0.6588548816627253 0.162677948329\n",
      "0.44689261122513846 0.066590117175\n",
      "0.049787430824688284 0.113427654128\n",
      "0.20693893014463327 0.0421869248182\n",
      "0.7102488753423684 0.185976138977\n",
      "0.3524093833534082 0.0237584923398\n",
      "0.3943965809899034 0.0427923456232\n",
      "0.23850492659127043 0.0278772642805\n",
      "0.7091047499064507 0.185457478126\n",
      "0.5877428939589111 0.130441094792\n",
      "0.14194280079487598 0.0716513058597\n",
      "0.22512167491426904 0.033944229051\n",
      "0.4870920136115232 0.0848135178821\n",
      "0.6249896012764282 0.14732596452\n",
      "0.14926783625676565 0.0683306829526\n",
      "0.8492363973321782 0.248982680273\n",
      "0.9899273515137473 0.312761430247\n",
      "0.3253178896198541 0.0114772364815\n",
      "0.2578315575983736 0.0191160160963\n",
      "0.3618709775411234 0.0280476710832\n",
      "0.5410398471911569 0.109269428424\n",
      "0.25605066945682364 0.0199233375064\n",
      "0.10868388456096156 0.0867284095387\n",
      "0.5531033995960241 0.114738140305\n",
      "0.15012770043371304 0.067940884883\n",
      "0.25254259085069874 0.0215136378182\n",
      "0.31680316892106253 0.00761729931841\n",
      "0.5446768540034782 0.110918175136\n",
      "0.013236017793516908 0.12999732946\n",
      "0.6164774660368251 0.143467199411\n",
      "0.25791352639607235 0.0190788575776\n",
      "0.3335159641554156 0.0151936299704\n",
      "0.66934259636111 0.167432293321\n",
      "0.9439522366762598 0.291919753742\n",
      "0.5277311953751929 0.103236281648\n",
      "0.8959144475714083 0.270143015085\n",
      "0.892193637097365 0.268456278064\n",
      "0.582268627171297 0.127959471898\n",
      "0.9302068692827258 0.285688632805\n",
      "0.39916530548063234 0.0449541284383\n",
      "0.30053721152837887 0.000243531504574\n",
      "0.734858291348355 0.197132206541\n",
      "0.36341074349253444 0.0287456857367\n",
      "0.08504823890402546 0.0974430424987\n",
      "0.9453066505336095 0.292533743627\n",
      "0.7184932719782207 0.18971353144\n",
      "0.2437294803783826 0.0255088425745\n",
      "0.16991917441908155 0.0589689116791\n",
      "0.7555578098147865 0.206515819159\n",
      "0.3842686264289332 0.0382010889539\n",
      "0.3002237768154994 0.000101443661739\n",
      "0.4816454167689239 0.0823444384715\n",
      "0.43053938254872404 0.0591767870921\n",
      "0.3370128105788617 0.0167788384512\n",
      "0.08439170525838446 0.0977406657217\n",
      "0.21477337788245565 0.0386353725072\n",
      "0.3683140422819501 0.0309684744679\n",
      "0.8584669742161183 0.253167133059\n",
      "0.3921806723905753 0.0417878184924\n",
      "0.08799241680461778 0.096108372567\n",
      "0.3805468487477798 0.0365139134736\n",
      "0.8444789711293833 0.246826019256\n",
      "0.18626404355252546 0.0515593711874\n",
      "0.2900124343153684 0.00452761485881\n",
      "0.5834539332888641 0.128496800989\n",
      "0.2706349870896758 0.0133118993136\n",
      "0.16629873090144953 0.060610149833\n",
      "0.2292530192083897 0.0320713867185\n",
      "0.9992308587984617 0.316978944219\n",
      "0.8605010080767554 0.254089211794\n",
      "0.2982920882865776 0.000774239358759\n",
      "0.6018955827189264 0.136856864754\n",
      "0.7605572130555696 0.208782174456\n",
      "0.15804007194310044 0.0643540077654\n",
      "0.9886691258588693 0.312191044895\n",
      "0.438320472630525 0.0627041510349\n",
      "0.35506047384603334 0.024960298374\n",
      "0.9647199299304856 0.301334271707\n",
      "0.9779415851254301 0.307327980725\n",
      "0.01523631536771941 0.129090544233\n",
      "0.9880418880163953 0.311906702197\n",
      "0.8944728789003169 0.26948951573\n",
      "0.8767107574383097 0.261437499093\n",
      "0.7911204831512619 0.222637273906\n",
      "0.35811244964693734 0.0263438358066\n",
      "0.11567485638251596 0.0835592260864\n",
      "0.661120713196921 0.163705106782\n",
      "0.597476996680359 0.134853808511\n",
      "0.2917635520114118 0.00373378914087\n",
      "0.14778584752591462 0.0690025057381\n",
      "0.6652706625903019 0.165586383274\n",
      "0.7997749040639893 0.226560540691\n",
      "0.9029131554867968 0.273315705503\n",
      "0.5143831427015538 0.0971852734714\n",
      "0.17529515184868183 0.0565318458255\n",
      "0.48469310888555306 0.0837260340022\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w = np.random.uniform(low=0.0, high=1.0)\n",
    "    y_predict = w * x\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    print(w, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:15:05.945991Z",
     "start_time": "2018-01-08T15:15:05.889524Z"
    },
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "100번의 회차(epoch)동안,\n",
    "크기가 100인 배열 `x`에 랜덤한 값 100개가 들어 있는 `w`를 곱합니다.\n",
    "`y_predict`에는 이 곱의 결과가 저장되어 있습니다. 역시 크기가 100인 배열입니다.\n",
    "```python\n",
    "y_predict = w * x\n",
    "```\n",
    "\n",
    "`y_predict`와 `y`의 차이를 계산하면, 100개의 차이값을 구할 수 있습니다.\n",
    "```python\n",
    "y_predict - y\n",
    "```\n",
    "\n",
    "우리가 관심 있는 것은 '얼만큼 큰 차이가 나고 있는지'이지, `y_predict`의 원소가 더 컸는지 `y`의 원소가 더 컸는지가 아닙니다. 각각의 차이가 음수인지 양수인지는 관심이 없기 때문에, 절대값을 구하는 `np.abs` 함수를 이용해서 모든 차이를 양수로 변환합니다.\n",
    "```python\n",
    "np.abs(y_predict - y)\n",
    "```\n",
    "\n",
    "100개나 되는 원소들의 차이를 일일히 들고 있으려면 복잡하기 때문에, 이들의 평균값을 대표로 들고 있겠습니다.\n",
    "```python\n",
    "error = np.abs(y_predict - y).mean()\n",
    "```\n",
    "\n",
    "결국 매 회차마다 우리가 계산한 것은 \"임의의 값을 데이터 `x`에 곱했을 때 실제 값 `y`와의 차의 절대값의 평균\"이 되는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "이제 위 코드를 이용해서, 매 회차마다 '이전보다 더 결과가 잘 나온 값'만을 보관하도록 하겠습니다.\n",
    "\n",
    "100번쯤 돌고 나면 아마 충분히 좋은 결과만 남아 있겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:10:58.583176Z",
     "start_time": "2018-01-08T15:10:58.562241Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w = 0.651733, error = 0.159449\n",
      " 1 w = 0.444996, error = 0.065730\n",
      " 4 w = 0.279542, error = 0.009274\n",
      "72 w = 0.291448, error = 0.003877\n",
      "77 w = 0.298021, error = 0.000897\n",
      "96 w = 0.299953, error = 0.000021\n",
      "----------------------------------------\n",
      "99 w = 0.2999531451417199, error = 0.000021\n"
     ]
    }
   ],
   "source": [
    "# best_error 값을 들고 있으면서, 새로 계산한 error와 계속해서 비교한다.\n",
    "# 루프가 끝날 때 저장되어 있는 값이 가장 적당한 값이다.\n",
    "\n",
    "num_epoch = 100\n",
    "best_error = 9999 # ※ (1)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w = np.random.uniform(low=0.0, high=1.0)\n",
    "    y_predict = w * x\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    # 이제까지 나온 최고 값과 비교한다.\n",
    "    if error < best_error:  # ※ (2)\n",
    "        best_w = w\n",
    "        best_error = error\n",
    "        \n",
    "        print(f\"{epoch:2} w = {best_w:.6}, error = {best_error:.6f}\")\n",
    "    \n",
    "# ※ (3)\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w = {best_w:2}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "먼저번의 코드와 비교해서 2군데 수정사항이 생겼습니다.\n",
    "\n",
    "**※ (1)**\n",
    "\n",
    "먼저 `best_error`라는 변수를 새로 만들고, 앞으로 가장 작은 에러만 저장하고 있도록 하겠습니다. 물론 처음의 값은 아주 큰 값이어야 합니다.\n",
    "\n",
    "**※ (2)**\n",
    "\n",
    "매 회차마다 계산한 `error`를 `best_error`와 비교합니다. 만약에 새로 계산한 `error`가 기존의 최고결과보다도 더 오차가 적다고 나오면, 최고 기록을 경신할 필요가 있습니다. `best_error` 만 수정할 뿐이 아니라 최고값을 만든 `w`도 저장해 둡니다.\n",
    "\n",
    "**※ (3)**\n",
    "\n",
    "주어진 모든 회차를 모두 돌고 나면, 최고기록을 내게 한 `w`와 그때의 `error`를 알 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "조금 더 개선해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w = 0.492455, error = 0.087245\n",
      " 1 w = 0.247129, error = 0.023968\n",
      " 6 w = 0.270048, error = 0.013578\n",
      " 7 w = 0.276200, error = 0.010789\n",
      "26 w = 0.309792, error = 0.004439\n",
      "----------------------------------------\n",
      "99 w = 0.309792, error = 0.004439\n"
     ]
    }
   ],
   "source": [
    "# 적당한 타이밍에 스탑한다.\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "best_error = 9999\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w = np.random.uniform(low=0.0, high=1.0)\n",
    "    y_predict = w * x\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.003: # ※ (1)\n",
    "        break\n",
    "    \n",
    "    # 이제까지 나온 최고 값과 비교한다.\n",
    "    if error < best_error:\n",
    "        best_w = w\n",
    "        best_error = error\n",
    "        \n",
    "        print(f\"{epoch:2} w = {best_w:.6f}, error = {best_error:.6f}\")\n",
    "    \n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w = {best_w:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "**※ (1)**\n",
    "\n",
    "우리가 판단하기에 `error`가 이제 충분히 작아졌다고 생각이 든다면, 굳이 남은 회차를 모두 돌 필요 없이 중간에 루프를 빠져나오도록 합니다.\n",
    "\n",
    "이 경우에는 에러가 `0.003`보다 작으면 충분하다고 판단했습니다.\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epoch):\n",
    "    ...\n",
    "    if error < 0.003:\n",
    "        break\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "이상이 무작위 검색(random search)을 이용해서 y의 값을 추정하는 방식입니다.\n",
    "구현 방식이 단순하(고 무식하)다는 데에 그 특징이 있습니다.\n",
    "\n",
    "그렇다면 이 방법의 **문제점**은 무엇일까요?\n",
    "\n",
    "문제점 중 하나로, 만약에 `y`에 붙은 상수가 `0.3`이 아니라 더 큰 값 -- 예컨대, 100 -- 이라면,\n",
    "단순히 0과 1 사이의 랜덤한 값으로는 찾을 수 없다는 데에 있습니다.\n",
    "\n",
    "`w`를 구하는 데에 더 큰 랜덤 값을 써야 합니다. 그러면 그만큼 정확도 떨어지게 되고, 답을 구하는 데 더 오래 걸리거나 부정확한 결과가 나올 수 있습니다.\n",
    "\n",
    "다음에는 무작위 방식보다 조금 더 똑똑한 방식을 써보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### Case 2 - h-step search\n",
    "\n",
    "다음 방법으로, \"h-step search\" 방식이라는 것을 써보도록 하겠습니다.\n",
    "h-step search는 매번 랜덤하게 `w`를 다시 구하는 것이 아니라, 가급적 이전에 구한 `w` 중 괜찮은 값을 보존하려고 노력합니다.\n",
    "\n",
    "**h-step search 전략: 에러가 적게 난 방향을 찾아서 한 발자국씩 이동한다.**\n",
    "\n",
    "1. 어느 '방향'이 에러가 적은지 판단한다.\n",
    "2. 한번에 그 방향으로 한 발자국씩 `h` 만큼 이동한다.\n",
    "\n",
    "※ **NOTE** h-step은 나중에 나올 Gradient search 방법으로 가는 사이에 임시로 만난 방식입니다. (실제 존재하진 않아요~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:25:39.440835Z",
     "start_time": "2018-01-08T15:25:39.350909Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6151403405242338 0.142861046768\n",
      "1 0.6151403405242338 0.142861046768\n",
      "2 0.6151403405242338 0.142861046768\n",
      "3 0.6151403405242338 0.142861046768\n",
      "4 0.6151403405242338 0.142861046768\n",
      "5 0.6151403405242338 0.142861046768\n",
      "6 0.6151403405242338 0.142861046768\n",
      "7 0.6151403405242338 0.142861046768\n",
      "8 0.6151403405242338 0.142861046768\n",
      "9 0.6151403405242338 0.142861046768\n",
      "10 0.6151403405242338 0.142861046768\n",
      "11 0.6151403405242338 0.142861046768\n",
      "12 0.6151403405242338 0.142861046768\n",
      "13 0.6151403405242338 0.142861046768\n",
      "14 0.6151403405242338 0.142861046768\n",
      "15 0.6151403405242338 0.142861046768\n",
      "16 0.6151403405242338 0.142861046768\n",
      "17 0.6151403405242338 0.142861046768\n",
      "18 0.6151403405242338 0.142861046768\n",
      "19 0.6151403405242338 0.142861046768\n",
      "20 0.6151403405242338 0.142861046768\n",
      "21 0.6151403405242338 0.142861046768\n",
      "22 0.6151403405242338 0.142861046768\n",
      "23 0.6151403405242338 0.142861046768\n",
      "24 0.6151403405242338 0.142861046768\n",
      "25 0.6151403405242338 0.142861046768\n",
      "26 0.6151403405242338 0.142861046768\n",
      "27 0.6151403405242338 0.142861046768\n",
      "28 0.6151403405242338 0.142861046768\n",
      "29 0.6151403405242338 0.142861046768\n",
      "30 0.6151403405242338 0.142861046768\n",
      "31 0.6151403405242338 0.142861046768\n",
      "32 0.6151403405242338 0.142861046768\n",
      "33 0.6151403405242338 0.142861046768\n",
      "34 0.6151403405242338 0.142861046768\n",
      "35 0.6151403405242338 0.142861046768\n",
      "36 0.6151403405242338 0.142861046768\n",
      "37 0.6151403405242338 0.142861046768\n",
      "38 0.6151403405242338 0.142861046768\n",
      "39 0.6151403405242338 0.142861046768\n",
      "40 0.6151403405242338 0.142861046768\n",
      "41 0.6151403405242338 0.142861046768\n",
      "42 0.6151403405242338 0.142861046768\n",
      "43 0.6151403405242338 0.142861046768\n",
      "44 0.6151403405242338 0.142861046768\n",
      "45 0.6151403405242338 0.142861046768\n",
      "46 0.6151403405242338 0.142861046768\n",
      "47 0.6151403405242338 0.142861046768\n",
      "48 0.6151403405242338 0.142861046768\n",
      "49 0.6151403405242338 0.142861046768\n",
      "50 0.6151403405242338 0.142861046768\n",
      "51 0.6151403405242338 0.142861046768\n",
      "52 0.6151403405242338 0.142861046768\n",
      "53 0.6151403405242338 0.142861046768\n",
      "54 0.6151403405242338 0.142861046768\n",
      "55 0.6151403405242338 0.142861046768\n",
      "56 0.6151403405242338 0.142861046768\n",
      "57 0.6151403405242338 0.142861046768\n",
      "58 0.6151403405242338 0.142861046768\n",
      "59 0.6151403405242338 0.142861046768\n",
      "60 0.6151403405242338 0.142861046768\n",
      "61 0.6151403405242338 0.142861046768\n",
      "62 0.6151403405242338 0.142861046768\n",
      "63 0.6151403405242338 0.142861046768\n",
      "64 0.6151403405242338 0.142861046768\n",
      "65 0.6151403405242338 0.142861046768\n",
      "66 0.6151403405242338 0.142861046768\n",
      "67 0.6151403405242338 0.142861046768\n",
      "68 0.6151403405242338 0.142861046768\n",
      "69 0.6151403405242338 0.142861046768\n",
      "70 0.6151403405242338 0.142861046768\n",
      "71 0.6151403405242338 0.142861046768\n",
      "72 0.6151403405242338 0.142861046768\n",
      "73 0.6151403405242338 0.142861046768\n",
      "74 0.6151403405242338 0.142861046768\n",
      "75 0.6151403405242338 0.142861046768\n",
      "76 0.6151403405242338 0.142861046768\n",
      "77 0.6151403405242338 0.142861046768\n",
      "78 0.6151403405242338 0.142861046768\n",
      "79 0.6151403405242338 0.142861046768\n",
      "80 0.6151403405242338 0.142861046768\n",
      "81 0.6151403405242338 0.142861046768\n",
      "82 0.6151403405242338 0.142861046768\n",
      "83 0.6151403405242338 0.142861046768\n",
      "84 0.6151403405242338 0.142861046768\n",
      "85 0.6151403405242338 0.142861046768\n",
      "86 0.6151403405242338 0.142861046768\n",
      "87 0.6151403405242338 0.142861046768\n",
      "88 0.6151403405242338 0.142861046768\n",
      "89 0.6151403405242338 0.142861046768\n",
      "90 0.6151403405242338 0.142861046768\n",
      "91 0.6151403405242338 0.142861046768\n",
      "92 0.6151403405242338 0.142861046768\n",
      "93 0.6151403405242338 0.142861046768\n",
      "94 0.6151403405242338 0.142861046768\n",
      "95 0.6151403405242338 0.142861046768\n",
      "96 0.6151403405242338 0.142861046768\n",
      "97 0.6151403405242338 0.142861046768\n",
      "98 0.6151403405242338 0.142861046768\n",
      "99 0.6151403405242338 0.142861046768\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "h = 0.1 # ※ (1)\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    print(epoch, w, current_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "아직까지는 이전 random search와 크게 달라지지 않았습니다.\n",
    "\n",
    "차이가 있다면 먼저 한발자국으로 얼마만큼 이동할지 `h`의 값을 우리가 정해주었다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:56:54.543682Z",
     "start_time": "2018-01-08T15:56:54.522052Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 w = 0.313438, error=0.006091558273330273\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "h = 0.1 # ※\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "# w = 0.0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    # 미래로 한발자국 더 가서 비교해본다. ※ (1)\n",
    "    y_predict = (w + h) * x # y를 한번 더 계산해 본다.\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if h_plus_error < current_error:\n",
    "        w = w + h    \n",
    "#         print(f\"{epoch:2} w = {w:.6f}, error={current_error}, {h_plus_error}\")\n",
    "    \n",
    "print(f\"{epoch:2} w = {w:.6f}, error={current_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:56:52.598167Z",
     "start_time": "2018-01-08T15:56:52.556977Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 w = 0.296263, error = 0.001694\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "h = 0.01\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "# w = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if current_error < 0.001: # ※\n",
    "        break\n",
    "\n",
    "    # +h만큼 이동해본다\n",
    "    y_predict = (w + h) * x\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        w = w + h\n",
    "        continue\n",
    "        \n",
    "    # 반대 방향으로 가본다\n",
    "    y_predict = (w - h) * x\n",
    "    h_minus_error = np.abs(y_predict - y).mean()\n",
    "    if h_minus_error < current_error:\n",
    "        w = w - h\n",
    "        continue\n",
    "        \n",
    "    # 어느 쪽으로 가도 결과가 좋아지지 않는다면, 이미 최적의 위치에 도달했다는 얘기겠네요.\n",
    "    # 이만 루프를 끝냅니다.\n",
    "    break\n",
    "       \n",
    "print(f\"{epoch:2} w = {w:.6f}, error = {current_error:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "h-step 방식에서는 매 회차마다 `w`를 새로 계산하지 않고, 이전에 계산했던 `w`를 조금씩 수정해 가면서 사용합니다.\n",
    "\n",
    "얼마만큼씩 수정하면 좋을까요? 에러가 컸을 때는 크게, 에러가 작을 때는 작게 수정하는 것이 좋을 것입니다.\n",
    "\n",
    "하지만 에러값은 이미 원소들 각각의 오차가 아닌, 그것들의 평균값으로 치환되어 버렸습니다. 각각의 `w` 원소들에게 일괄적으로 그 평균 에러를 빼버리면 어떻게 될까요?\n",
    "\n",
    "원소에 따라서는 실제 `y` 원소에 가깝게 다가가는 원소도 있겠지만, 어떤 원소는 너무 조금 다가가기도 할 것입니다. 더 나쁜 경우는 실제 `y` 값을 지나쳐서 가버릴 수도 있습니다. 그 원소에게는 에러가 너무 컸던 것입니다.\n",
    "\n",
    "따라서 우리가 정한 한발자국 `h` 만큼 이동해 봅니다. 양수만큼 이동하는 것이 좋을지, 음수만큼 이동하는 것이 좋을지 모르니 양쪽 방향으로 다 해봅니다.\n",
    "현재의 에러보다 작아지면 값이 나오면, 성공입니다.\n",
    "\n",
    "실행해보면 100번을 모두 돌기 전에 충분히 에러가 작아져서 루프를 끝내고 나오는 것을 확인할 수 있습니다. `w`의 초기값이 임의로 정해지기 때문에 실행할 때마다 회차수는 달라지지만, 대부분 99에 미치지 못하고 그 전에 끝나고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T15:51:40.674156Z",
     "start_time": "2018-01-08T15:51:40.563014Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1110ba5f8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKPRFRELkfwFj3MmjH9ainwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1110ba5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict = w * x\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_predict, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "`w`를 제대로 구했는지, 실제 `x`, `y` 데이터 위로 우리의 추정선을 그어 보도록 하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "h-step search는 매 회차마다 `w`를 완전히 새로 계산하는 것이 아니라, 이전에 계산했던 값을 기반으로 필요한만큼만 업데이트하기 때문에 시간을 절약한다는 장점이 있습니다.\n",
    "\n",
    "하지만 이 방법으로는 `h`를 얼마로 정해야 할지 모른다는 단점이 있습니다. `h`를 너무 작게 정할 경우, 너무 오래 걸릴 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### Case 3 - Gradient Descent (not yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "이번에는 error의 크기를 보고 그에 맞추어서 `h`의 값을 유기적으로 바꾸어보도록 하겠습니다. 이 방법이 성공한다면 `h`의 값을 얼마로 정하는 것이 바람직할지 고민을 하지 않아도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:06:42.057207Z",
     "start_time": "2018-01-08T16:06:42.033156Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w = 0.321944, error = 0.018196\n",
      " 1 w = 0.311996, error = 0.009948\n",
      " 2 w = 0.306558, error = 0.005438\n",
      " 3 w = 0.303585, error = 0.002973\n",
      " 4 w = 0.301960, error = 0.001625\n",
      "----------------------------------------\n",
      " 5 w = 0.301960, error = 0.000888\n"
     ]
    }
   ],
   "source": [
    "# error의 크기를 이용해서 h를 변화시키는 정도를 결정한다.\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    # w 값을 바로 수정합니다.\n",
    "    w = w - (y_predict - y).mean() # ※ (1)\n",
    "\n",
    "    print(f\"{epoch:2} w = {w:.6f}, error = {error:6f}\")\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w = {w:.6f}, error = {error:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "**※ (1)**\n",
    "\n",
    "각 원소들의 에러 `y_predict - y`의 평균을 구하고 그것을 `w`에서 빼주고 있습니다.\n",
    "\n",
    "위의 h-step 방식과 달리, 양의 방향과 음의 방향을 따로 계산하고 있지 않습니다. `y_predict`가 대부분 `y`보다 크다면 에러의 평균은 양수값이 되고, 그러면 `w`를 줄이면 됩니다.\n",
    "반대로 대부분의 `y_predict`보다 `y`가 크다면 에러의 평균은 음수값이 되고, `w`에서 음수를 빼면 결과적으로 `w` 값이 더 커지게 됩니다.\n",
    "\n",
    "결과가 아주 빨리 수렴하고 있습니다. 10번이 채 걸리지 않고 있습니다.\n",
    "\n",
    "※ **NOTE**\n",
    "\n",
    "절대값이 씌워진 `np.abs(y_predict - y).mean()`과 <br> 절대값이 없는 `(y_predict - y).mean()`의 차이에 주의하세요.\n",
    "\n",
    "- `np.abs(y_predict - y).mean()`: 에러의 양의 크기의 평균. 에러에서 방향에 대한 정보는 지우고, 얼마나 참값에서 벗어나 있는지 그 크기만을 나타낸다.\n",
    "- `(y_predict - y).mean()`: 에러들의 평균. 방향까지 감안해서 에러들이 평균적으로 벗어난 정도를 가늠한다. 각 원소들의 에러끼리 상쇄되는 효과까지 포함한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 변수가 여러개인 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:08:08.481864Z",
     "start_time": "2018-01-08T16:08:08.473025Z"
    }
   },
   "source": [
    "`y`를 나타내는 변수가 하나보다 많으면 어떻게 될까요?\n",
    "\n",
    "`y`를 `x1`과 `x2` 두 개의 변수로 표현하는 경우에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset (2 var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:09:20.921225Z",
     "start_time": "2018-01-08T16:09:20.911019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.05094007,  0.19210054,  0.82724913,  0.73087288])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "##\n",
    "print(x1.shape)\n",
    "x1[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "랜덤한 값 100개를 만들어 **변수 `x1`**에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:09:23.607730Z",
     "start_time": "2018-01-08T16:09:23.596798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.75048761,  0.03009296,  0.32424234,  0.64926923])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "##\n",
    "print(x2.shape)\n",
    "x2[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "마찬가지로 **변수 `x2`**에 랜덤한 값 100개를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:10:55.726709Z",
     "start_time": "2018-01-08T16:10:55.717031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "x1 = [ 0.05094007  0.19210054  0.82724913  0.73087288]\n",
      "x2 = [ 0.75048761  0.03009296  0.32424234  0.64926923]\n",
      "y  = [ 0.39052583  0.07267664  0.41029591  0.54389648]\n"
     ]
    }
   ],
   "source": [
    "y = 0.3 * x1 + 0.5 * x2\n",
    "\n",
    "##\n",
    "print(y.shape)\n",
    "print('x1 =', x1[0:4])\n",
    "print('x2 =', x2[0:4])\n",
    "print('y  =', y[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "`y`는 이 `x1`과 `x2`의 조합으로 나타납니다.\n",
    "\n",
    "```python\n",
    "y = 0.3 * x1 + 0.5 * x2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 - Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 랜덤 서치부터 하나씩 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:14:21.404557Z",
     "start_time": "2018-01-08T16:14:21.273910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = 0.124609, w2 = 0.349701, error = 0.166696\n",
      " 2 w1 = 0.674719, w2 = 0.459286, error = 0.165143\n",
      " 3 w1 = 0.455682, w2 = 0.39116, error = 0.046870\n",
      "18 w1 = 0.151689, w2 = 0.620315, error = 0.044938\n",
      "37 w1 = 0.390097, w2 = 0.384504, error = 0.035893\n",
      "74 w1 = 0.350249, w2 = 0.450877, error = 0.016104\n",
      "294 w1 = 0.305709, w2 = 0.520694, error = 0.013863\n",
      "327 w1 = 0.270128, w2 = 0.529393, error = 0.009611\n",
      "1238 w1 = 0.287801, w2 = 0.524313, error = 0.008296\n",
      "1402 w1 = 0.32613, w2 = 0.481388, error = 0.007867\n",
      "3315 w1 = 0.292974, w2 = 0.498983, error = 0.004006\n",
      "4072 w1 = 0.290798, w2 = 0.502823, error = 0.003355\n",
      "----\n",
      "4999 w1 = 0.290798, w2 = 0.502823, error = 0.003355\n"
     ]
    }
   ],
   "source": [
    "# 적당한 타이밍에 스탑한다.\n",
    "\n",
    "num_epoch = 5000\n",
    "\n",
    "best_error = 9999\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0) # ※ 1\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.003:\n",
    "        break\n",
    "    \n",
    "    # 이제까지 나온 최고 값과 비교한다.\n",
    "    if error < best_error:\n",
    "        best_w1 = w1 # ※ 2\n",
    "        best_w2 = w2\n",
    "        best_error = error\n",
    "        \n",
    "        print(f\"{epoch:2} w1 = {best_w1:.6}, w2 = {best_w2:.6}, error = {best_error:.6f}\") # ※ 3\n",
    "    \n",
    "print(\"----\")\n",
    "print(f\"{epoch:2} w1 = {best_w1:.6}, w2 = {best_w2:.6}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "처음의 코드와 비교해서 변한 곳은:\n",
    "\n",
    "- **※ 1** - `w1`, `w2` 둘 다 계산합니다.\n",
    "- **※ 2** - 최고값과 비교해서 `btest_error`, `best_w1`, `best_w2`을 할당합니다.\n",
    "- **※ 3** - 출력\n",
    "\n",
    "그런데, 처음과 동일한 방식으로 접근했는데 계산 결과가 만족스럽지 않습니다. 에러가 `0.003` 밑으로는 어지간해서는 내려가지 않습니다. 심지어 회차 수를 5000번까지 늘려도 마찬가지입니다.\n",
    "\n",
    "그 이유는, 하나의 변수만 관계를 가지고 있을 때에 비해서 두 개의 변수가 생기자 너무 복잡해져서 단순히 랜덤한 값으로 맞추기가 어려워졌기 때문입니다.\n",
    "\n",
    "변수가 많아지면 실제에서는 랜덤 서치를 사용하기가 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - h-step search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임시 단계인 h-step search는 건너띄고 바로 gradient search를 향해 가도록 하겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3 - Gradient Descent (still not yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:23:21.672760Z",
     "start_time": "2018-01-08T16:23:21.623716Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = 0.118271, w2 = 0.667593 error = 0.056511\n",
      " 1 w1 = 0.118353, w2 = 0.667675 error = 0.056555\n",
      " 2 w1 = 0.118350, w2 = 0.667673 error = 0.056552\n",
      " 3 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 4 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 5 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 6 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 7 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 8 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      " 9 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "10 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "11 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "12 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "13 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "14 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "15 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "16 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "17 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "18 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "19 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "20 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "21 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "22 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "23 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "24 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "25 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "26 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "27 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "28 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "29 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "30 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "31 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "32 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "33 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "34 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "35 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "36 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "37 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "38 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "39 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "40 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "41 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "42 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "43 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "44 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "45 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "46 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "47 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "48 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "49 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "50 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "51 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "52 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "53 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "54 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "55 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "56 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "57 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "58 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "59 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "60 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "61 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "62 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "63 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "64 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "65 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "66 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "67 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "68 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "69 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "70 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "71 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "72 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "73 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "74 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "75 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "76 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "77 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "78 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "79 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "80 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "81 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "82 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "83 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "84 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "85 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "86 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "87 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "88 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "89 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "90 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "91 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "92 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "93 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "94 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "95 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "96 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "97 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "98 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "99 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n",
      "----------------------------------------\n",
      "99 w1 = 0.118350, w2 = 0.667673 error = 0.056553\n"
     ]
    }
   ],
   "source": [
    "# error의 크기를 이용해서 h를 변화시키는 정도를 결정한다.\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0) # ※ (1)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 # ※\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    w1 = w1 - (y_predict - y).mean() # ※ (2)\n",
    "    w2 = w2 - (y_predict - y).mean()\n",
    "\n",
    "    print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f} error = {error:6f}\") # ※\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f} error = {error:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "이 코드도 마찬가지로 `w1`, `w2`를 할당하는 부분(**※ 1**)과 갱신하는 부분(**※ 2**)이 수정되었습니다.\n",
    "\n",
    "그런데 결과가 이상합니다.\n",
    "\n",
    "어느 순간부터 더 이상 에러가 작아지지 않고 있습니다. 결국 100번의 회차가 모두 돌 때까지, 에러는 처음에만 조금 작아질 뿐 더 이상 개선되지 않았습니다. <br>\n",
    "그 이유는 무엇일까요?\n",
    "\n",
    "그 이유는 `w1`과 `w2` 두 개의 변수가 각기 다른 방향으로 차이가 나고 있기 때문입니다.\n",
    "\n",
    "이를테면 `w1`은 양의 방향으로 작아져야 하고, `w2`는 음의 방향으로 작아져야 하는 상황인데, 둘 다 모두 같이 `(y_predict - y).mean()` 작아지도록 하고 있기 때문에, 어느 한쪽은 좋아지지만 반대쪽이 나빠지는 것입니다.\n",
    "\n",
    "... 직교 좌표 내에 `(0.3, 0.5)` 위치에 도달해야 하는데,\n",
    "w1과 w2가 차이 나는 부분을 각기 적용해야 하는데\n",
    "둘이 동일하게 적용되니까 어느 순간 다른 한쪽은 지나쳐 버립니다.\n",
    "\n",
    "그럼 `w1`, `w2`에 알맞게 값을 다르게 적용하려면 어떻게 하면 될까요?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:26:38.546443Z",
     "start_time": "2018-01-08T16:26:38.513581Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = 0.259684, w2 = 0.298450 error = 0.314986\n",
      " 1 w1 = 0.324871, w2 = 0.379973 error = 0.127488\n",
      " 2 w1 = 0.347974, w2 = 0.415856 error = 0.053026\n",
      " 3 w1 = 0.354350, w2 = 0.433099 error = 0.027645\n",
      " 4 w1 = 0.354209, w2 = 0.442609 error = 0.020769\n",
      " 5 w1 = 0.351649, w2 = 0.448802 error = 0.018324\n",
      " 6 w1 = 0.348307, w2 = 0.453474 error = 0.016694\n",
      " 7 w1 = 0.344829, w2 = 0.457365 error = 0.015347\n",
      " 8 w1 = 0.341462, w2 = 0.460784 error = 0.014148\n",
      " 9 w1 = 0.338291, w2 = 0.463871 error = 0.013050\n",
      "10 w1 = 0.335340, w2 = 0.466690 error = 0.012038\n",
      "11 w1 = 0.332607, w2 = 0.469280 error = 0.011105\n",
      "12 w1 = 0.330082, w2 = 0.471664 error = 0.010244\n",
      "13 w1 = 0.327751, w2 = 0.473862 error = 0.009450\n",
      "14 w1 = 0.325601, w2 = 0.475889 error = 0.008717\n",
      "15 w1 = 0.323616, w2 = 0.477758 error = 0.008041\n",
      "16 w1 = 0.321785, w2 = 0.479483 error = 0.007418\n",
      "17 w1 = 0.320096, w2 = 0.481073 error = 0.006843\n",
      "18 w1 = 0.318539, w2 = 0.482541 error = 0.006312\n",
      "19 w1 = 0.317101, w2 = 0.483894 error = 0.005823\n",
      "20 w1 = 0.315776, w2 = 0.485143 error = 0.005372\n",
      "21 w1 = 0.314553, w2 = 0.486295 error = 0.004955\n",
      "22 w1 = 0.313424, w2 = 0.487357 error = 0.004571\n",
      "23 w1 = 0.312384, w2 = 0.488337 error = 0.004217\n",
      "24 w1 = 0.311424, w2 = 0.489241 error = 0.003890\n",
      "25 w1 = 0.310538, w2 = 0.490075 error = 0.003588\n",
      "26 w1 = 0.309721, w2 = 0.490845 error = 0.003310\n",
      "27 w1 = 0.308968, w2 = 0.491554 error = 0.003053\n",
      "28 w1 = 0.308272, w2 = 0.492209 error = 0.002817\n",
      "29 w1 = 0.307631, w2 = 0.492813 error = 0.002598\n",
      "30 w1 = 0.307039, w2 = 0.493370 error = 0.002397\n",
      "31 w1 = 0.306494, w2 = 0.493884 error = 0.002211\n",
      "32 w1 = 0.305990, w2 = 0.494358 error = 0.002040\n",
      "33 w1 = 0.305526, w2 = 0.494796 error = 0.001882\n",
      "34 w1 = 0.305098, w2 = 0.495199 error = 0.001736\n",
      "35 w1 = 0.304702, w2 = 0.495571 error = 0.001601\n",
      "36 w1 = 0.304338, w2 = 0.495915 error = 0.001477\n",
      "37 w1 = 0.304002, w2 = 0.496231 error = 0.001363\n",
      "38 w1 = 0.303691, w2 = 0.496524 error = 0.001257\n",
      "39 w1 = 0.303405, w2 = 0.496793 error = 0.001159\n",
      "40 w1 = 0.303141, w2 = 0.497042 error = 0.001070\n",
      "----------------------------------------\n",
      "41 w1 = 0.303141, w2 = 0.497042 error = 0.000987\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    w1 = w1 - ((y_predict - y) * x1).mean() # ※ 1\n",
    "    w2 = w2 - ((y_predict - y) * x2).mean()\n",
    "\n",
    "    print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f} error = {error:6f}\") # ※\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f} error = {error:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "위의 코드와 비교하면 한 군데만 바뀌었습니다.\n",
    "\n",
    "**※ 1**\n",
    "\n",
    "`w1 = w1 - ((y_predict - y) `**` * x1`**`).mean()` <br>\n",
    "`w2 = w2 - ((y_predict - y) `**` * x2`**`).mean()`\n",
    "\n",
    "`y_predict`와 `y`의 차이에 각각 `x1`, `x2`를 곱해준 것입니다.\n",
    "\n",
    "에러가 점점 줄어들고, 마침내 충분한 값이 되어서 루프를 빠져나오고 있는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터가 아닌 일상적인 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 데이터 `x1`, `x2`에 따라 비례하는 관계가 아니라, 그와 무관하게 늘 값이 더해지는 경우를 살펴보겠습니다. <br>\n",
    "이를 치우침 혹은 편향(bias)이라고 합니다. 수학에서는 절편이라고 부르기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset (w/ bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:33:55.002918Z",
     "start_time": "2018-01-08T16:33:54.987535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.56489482,  0.44633331,  0.35026439,  0.76913603])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "# y에 bias(절편)을 추가한다.\n",
    "y = 0.3 * x1 + 0.5 * x2 + 0.1  # ※ 1\n",
    "\n",
    "##\n",
    "print(y.shape)\n",
    "y[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "`y`에는 이번에 `x1`, `x2` 뿐 아니라 `+ 0.1`도 붙었습니다.\n",
    "\n",
    "**※ 1**\n",
    "\n",
    "`y = 0.3 * x1 + 0.5 * x2`&nbsp;**` + 0.1`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 - Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:34:49.757629Z",
     "start_time": "2018-01-08T16:34:49.751560Z"
    }
   },
   "source": [
    "절편이라고 대처하는 방식이 다르지 않습니다. 어쨌거나 알고리즘에게는 역시 맞추어야 할 대상입니다.\n",
    "새로운 `w`가 나타난 것과 동일하게 대해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:35:02.278244Z",
     "start_time": "2018-01-08T16:35:02.157731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 w1 = 0.40957, w2 = 0.68889, b = 0.113086, error = 0.166845\n",
      "   8 w1 = 0.36444, w2 = 0.17680, b = 0.164739, error = 0.094491\n",
      "  10 w1 = 0.20308, w2 = 0.55807, b = 0.100040, error = 0.028530\n",
      " 197 w1 = 0.22921, w2 = 0.47592, b = 0.131070, error = 0.020749\n",
      "2259 w1 = 0.33894, w2 = 0.57798, b = 0.032473, error = 0.020065\n",
      "2682 w1 = 0.30532, w2 = 0.48265, b = 0.123268, error = 0.016962\n",
      "----------------------------------------\n",
      "4999 w1 = 0.30532, w2 = 0.48265, b = 0.521049, error = 0.016962\n"
     ]
    }
   ],
   "source": [
    "# 절편 b에 대한 값도 마찬가지로 랜덤하게 구한다.\n",
    "\n",
    "num_epoch = 5000\n",
    "best_error = 9999\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    b  = np.random.uniform(low=0.0, high=1.0) # ※ 1\n",
    "    \n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.003:\n",
    "        break\n",
    "    \n",
    "    # 이제까지 나온 최고 값과 비교한다.\n",
    "    if error < best_error:\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_b  = b # ※ 2\n",
    "        best_error = error\n",
    "        \n",
    "        print(f\"{epoch:4} w1 = {best_w1:.5f}, w2 = {best_w2:.5f}, b = {b:.6f}, error = {best_error:.6f}\") # ※ 3\n",
    "    \n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:4} w1 = {best_w1:.5f}, w2 = {best_w2:.5f}, b = {b:.6f}, error = {best_error:.6f}\") # ※ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- jupyter-side-memo -->\n",
    "\n",
    "**※ 1**\n",
    "\n",
    "`w1`, `w2`를 랜덤하게 생성할 때 마찬가지로 `b`도 랜덤값을 생성합니다.\n",
    "\n",
    "**※ 2**\n",
    "\n",
    "랜덤하게 생성한 값들이 최적의 에러값을 갱신했을 때, `best_w1`, `best_w2`와 더불어 `best_b`도 갱신해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3 - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:40:43.762949Z",
     "start_time": "2018-01-08T16:40:43.732741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = 0.398138, w2 = 0.105564, b = 0.161760, error = 0.187422\n",
      " 1 w1 = 0.438974, w2 = 0.182735, b = 0.254943, error = 0.120035\n",
      " 2 w1 = 0.398914, w2 = 0.174324, b = 0.194121, error = 0.087801\n",
      " 3 w1 = 0.404932, w2 = 0.210935, b = 0.218845, error = 0.081987\n",
      " 4 w1 = 0.386792, w2 = 0.220581, b = 0.196777, error = 0.071213\n",
      " 5 w1 = 0.383189, w2 = 0.243163, b = 0.200986, error = 0.068117\n",
      " 6 w1 = 0.372661, w2 = 0.256774, b = 0.191091, error = 0.062045\n",
      " 7 w1 = 0.366979, w2 = 0.273636, b = 0.189373, error = 0.058702\n",
      " 8 w1 = 0.359563, w2 = 0.287111, b = 0.183505, error = 0.054373\n",
      " 9 w1 = 0.353975, w2 = 0.300972, b = 0.180276, error = 0.051107\n",
      "10 w1 = 0.348182, w2 = 0.313235, b = 0.175918, error = 0.047652\n",
      "11 w1 = 0.343245, w2 = 0.325087, b = 0.172495, error = 0.044656\n",
      "12 w1 = 0.338521, w2 = 0.335959, b = 0.168849, error = 0.041739\n",
      "13 w1 = 0.334312, w2 = 0.346245, b = 0.165605, error = 0.039080\n",
      "14 w1 = 0.330401, w2 = 0.355801, b = 0.162403, error = 0.036560\n",
      "15 w1 = 0.326864, w2 = 0.364778, b = 0.159427, error = 0.034227\n",
      "16 w1 = 0.323617, w2 = 0.373157, b = 0.156564, error = 0.032035\n",
      "17 w1 = 0.320666, w2 = 0.381009, b = 0.153863, error = 0.030004\n",
      "18 w1 = 0.317973, w2 = 0.388351, b = 0.151285, error = 0.028108\n",
      "19 w1 = 0.315525, w2 = 0.395227, b = 0.148841, error = 0.026344\n",
      "20 w1 = 0.313299, w2 = 0.401661, b = 0.146515, error = 0.024693\n",
      "21 w1 = 0.311279, w2 = 0.407687, b = 0.144305, error = 0.023150\n",
      "22 w1 = 0.309447, w2 = 0.413328, b = 0.142203, error = 0.021706\n",
      "23 w1 = 0.307789, w2 = 0.418612, b = 0.140204, error = 0.020356\n",
      "24 w1 = 0.306291, w2 = 0.423560, b = 0.138303, error = 0.019092\n",
      "25 w1 = 0.304938, w2 = 0.428196, b = 0.136495, error = 0.017910\n",
      "26 w1 = 0.303720, w2 = 0.432539, b = 0.134775, error = 0.016803\n",
      "27 w1 = 0.302625, w2 = 0.436608, b = 0.133139, error = 0.015770\n",
      "28 w1 = 0.301644, w2 = 0.440422, b = 0.131582, error = 0.014808\n",
      "29 w1 = 0.300765, w2 = 0.443996, b = 0.130101, error = 0.013908\n",
      "30 w1 = 0.299982, w2 = 0.447347, b = 0.128691, error = 0.013064\n",
      "31 w1 = 0.299285, w2 = 0.450489, b = 0.127350, error = 0.012274\n",
      "32 w1 = 0.298667, w2 = 0.453435, b = 0.126073, error = 0.011535\n",
      "33 w1 = 0.298121, w2 = 0.456198, b = 0.124857, error = 0.010847\n",
      "34 w1 = 0.297642, w2 = 0.458789, b = 0.123699, error = 0.010203\n",
      "35 w1 = 0.297223, w2 = 0.461220, b = 0.122597, error = 0.009598\n",
      "36 w1 = 0.296858, w2 = 0.463502, b = 0.121548, error = 0.009031\n",
      "37 w1 = 0.296544, w2 = 0.465642, b = 0.120548, error = 0.008500\n",
      "38 w1 = 0.296275, w2 = 0.467652, b = 0.119597, error = 0.008001\n",
      "39 w1 = 0.296047, w2 = 0.469538, b = 0.118690, error = 0.007536\n",
      "40 w1 = 0.295856, w2 = 0.471309, b = 0.117826, error = 0.007100\n",
      "41 w1 = 0.295699, w2 = 0.472972, b = 0.117004, error = 0.006691\n",
      "42 w1 = 0.295573, w2 = 0.474534, b = 0.116220, error = 0.006308\n",
      "43 w1 = 0.295474, w2 = 0.476001, b = 0.115473, error = 0.005948\n",
      "44 w1 = 0.295399, w2 = 0.477380, b = 0.114762, error = 0.005610\n",
      "45 w1 = 0.295348, w2 = 0.478675, b = 0.114083, error = 0.005293\n",
      "46 w1 = 0.295316, w2 = 0.479893, b = 0.113437, error = 0.004994\n",
      "47 w1 = 0.295302, w2 = 0.481037, b = 0.112821, error = 0.004713\n",
      "48 w1 = 0.295303, w2 = 0.482113, b = 0.112234, error = 0.004451\n",
      "49 w1 = 0.295319, w2 = 0.483125, b = 0.111674, error = 0.004204\n",
      "50 w1 = 0.295348, w2 = 0.484077, b = 0.111141, error = 0.003974\n",
      "51 w1 = 0.295388, w2 = 0.484972, b = 0.110632, error = 0.003757\n",
      "52 w1 = 0.295437, w2 = 0.485814, b = 0.110147, error = 0.003554\n",
      "53 w1 = 0.295495, w2 = 0.486607, b = 0.109685, error = 0.003362\n",
      "54 w1 = 0.295560, w2 = 0.487353, b = 0.109244, error = 0.003181\n",
      "55 w1 = 0.295632, w2 = 0.488055, b = 0.108823, error = 0.003011\n",
      "56 w1 = 0.295709, w2 = 0.488716, b = 0.108422, error = 0.002851\n",
      "57 w1 = 0.295791, w2 = 0.489339, b = 0.108040, error = 0.002702\n",
      "58 w1 = 0.295876, w2 = 0.489925, b = 0.107675, error = 0.002561\n",
      "59 w1 = 0.295965, w2 = 0.490478, b = 0.107327, error = 0.002428\n",
      "60 w1 = 0.296056, w2 = 0.490998, b = 0.106995, error = 0.002302\n",
      "61 w1 = 0.296149, w2 = 0.491489, b = 0.106678, error = 0.002182\n",
      "62 w1 = 0.296244, w2 = 0.491951, b = 0.106376, error = 0.002070\n",
      "63 w1 = 0.296340, w2 = 0.492387, b = 0.106088, error = 0.001964\n",
      "64 w1 = 0.296436, w2 = 0.492798, b = 0.105813, error = 0.001865\n",
      "65 w1 = 0.296533, w2 = 0.493186, b = 0.105551, error = 0.001771\n",
      "66 w1 = 0.296629, w2 = 0.493552, b = 0.105300, error = 0.001682\n",
      "67 w1 = 0.296725, w2 = 0.493897, b = 0.105061, error = 0.001598\n",
      "68 w1 = 0.296821, w2 = 0.494222, b = 0.104833, error = 0.001520\n",
      "69 w1 = 0.296916, w2 = 0.494529, b = 0.104616, error = 0.001445\n",
      "70 w1 = 0.297010, w2 = 0.494819, b = 0.104408, error = 0.001375\n",
      "71 w1 = 0.297102, w2 = 0.495093, b = 0.104210, error = 0.001308\n",
      "72 w1 = 0.297193, w2 = 0.495352, b = 0.104021, error = 0.001245\n",
      "73 w1 = 0.297283, w2 = 0.495596, b = 0.103840, error = 0.001186\n",
      "74 w1 = 0.297371, w2 = 0.495827, b = 0.103668, error = 0.001129\n",
      "75 w1 = 0.297457, w2 = 0.496045, b = 0.103503, error = 0.001075\n",
      "76 w1 = 0.297542, w2 = 0.496251, b = 0.103346, error = 0.001024\n",
      "----------------------------------------\n",
      "77 w1 = 0.297542, w2 = 0.496251, b = 0.103346, error = 0.000975\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 + b # ※\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    w1 = w1 - ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - ((y_predict - y) * x2).mean()\n",
    "    b  = b - (y_predict - y).mean()\n",
    "\n",
    "    print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:6f}\") # ※\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T16:41:10.277110Z",
     "start_time": "2018-01-08T16:41:10.247336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "75 w1 = 0.303768, w2 = 0.502416, b = 0.096736, error = 0.000958\n"
     ]
    }
   ],
   "source": [
    "# learning_rate\n",
    "\n",
    "num_epoch = 100\n",
    "learning_rate = 1.1  # ※\n",
    "\n",
    "# w1, w2, b = 0, 0, 0\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b  = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 + b  # ※\n",
    "\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "\n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    w1 = w1 - learning_rate * ((y_predict - y) * x1).mean() # ※\n",
    "    w2 = w2 - learning_rate * ((y_predict - y) * x2).mean()\n",
    "    b  = b  - learning_rate * (y_predict - y).mean()\n",
    "\n",
    "#     print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:6f}\")  # ※\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(f\"{epoch:2} w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:32:35.970304Z",
     "start_time": "2018-01-11T09:32:35.964781Z"
    }
   },
   "source": [
    "# 이론\n",
    "\n",
    "## 용어 설명\n",
    "\n",
    "Machine Learning에는 크게 세 가지 타입의 알고리즘이 있습니다.\n",
    "\n",
    "  * **Supervised Learning**: 한국어로 **지도 학습**이라고 표현합니다. 우리가 맞춰야 하는 정답(Label)과, 정답을 맞추는데 도움이 되는 특징(Feature)을 보유하고 있는 데이터가 있으면, 이 데이터는 Supervised learning으로 풀 수 있습니다.\n",
    "  * **Unupervised Learning**: 한국어로 **비지도 학습**이라고 표현합니다. 이 경우는 Feature는 있지만 Label은 보유하고 있지 않습니다.\n",
    "  * **Reinforcement Learning**: 한국어로 **강화 학습**이라고 합니다. (알파고의 알고리즘이 Reinforcement Learning으로 만들어졌습니다) Supervised learning과 Unsupervised learning와는 약간 다른 개념이며, 이번 수업에서는 깊게 다루지 않습니다.\n",
    "  \n",
    "딥러닝은 세 가지 타입 모두에 적용할 수 있으나, 주로 supervised learning에서 많이 쓰입니다. 또한 딥러닝은 supervised learning 알고리즘 중 하나인 인공신경망(Artificial Neural Network, ANN) 알고리즘의 한 부류라고 보시면 됩니다.\n",
    "\n",
    "또한 이론을 더 쉽게 설명하기 위해, 다음의 표기를 사용하도록 하겠습니다.\n",
    "\n",
    "  * $x$ - Feature입니다. 줄여서 $x$라고 표현합니다.\n",
    "  * $y$ - Label입니다. 줄여서 $y$라고 표현합니다.\n",
    "  * $w$ - weight입니다. feature와 label의 관계를 표현하는 가중치라고 이해하시면 됩니다.\n",
    "  * $m$ - 데이터의 갯수입니다.\n",
    "    \n",
    "**우리의 목표는 언제나 가장 적합한 $w$(weight)를 찾는 것입니다.** 여기에 하나의 표기를 더 추가하겠습니다.\n",
    "\n",
    "  * **$h(x) $** - 가설 함수(hypothesis function)입니다. x가 주어졌을 때 **$h(x) $** = $w$ x $x$ 라고 생각합니다. 이 **$h(x) $**의 결과값이 $y$와 정확하게 일치하면 우리는 가장 좋은 $w$를 찾았다고 이해할 수 있습니다.\n",
    "  \n",
    "## Loss Function\n",
    "\n",
    "오늘의 핵심입니다. Loss Function은 우리의 가설(**$h(x)$**)이 정답($y$)에 얼마나 근접했는지를 정량적으로 나타내는 공식입니다. Loss Function이 되기 위해서는 다음의 세 가지 조건이 필요합니다.\n",
    "\n",
    "  1. 가설이 정답과 근접할수록 Loss Function의 결과는 0에 가까워져야 합니다.\n",
    "  2. 가설이 정답에서 멀어질수록 Loss Function의 결과는 0에서 멀어져야 합니다.\n",
    "  3. Loss Function은 편미분 가능해야 합니다.\n",
    "  \n",
    "이번 문제에서 사용할 우리의 Loss Function은 **Mean Squared Error(MSE)**이며, 구체적인 공식은 다음과 같습니다.\n",
    "\n",
    "$$ \n",
    "L(x) = \\frac{1}{2m}  \\sum_{i=0}^{m} {\\big( h(x^{(i)}) - y^{(i)} \\big)}^2\n",
    "$$\n",
    "\n",
    "우리는 이 Loss Function을 편미분함으로서 1) weight를 어느 방향으로 업데이트 해줘야 하는지(음수 or 양수), 2) weight를 얼만큼 크게 업데이트해줘야 하는지 알 수 있습니다. 즉, 우리는 Loss Function의 편미분 한 결과값을 그대로 weight에 업데이트해주는 것 만으로 가장 적합한 weight에 도달할 수 있습니다. 이 방식을 **Gradient Descent**라고 합니다. 먼저 Loss Function을 편미분 해줍니다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta  L(x)} {\\delta w} & = \\frac {1} {2m} 2 \\sum_{i=0}^{m} \\big(h(x^{(i)}) - y^{(i)}\\big) \\cdot \\frac{\\delta}{\\delta w} \\big(h(x^{(i)}) - y^{(i)}\\big) \\\\\n",
    "& = \\frac {1}{m} \\cdot \\sum_{i=0}^{m} \\big(h(x^{(i)}) - y^{(i)}\\big) \\cdot \\frac{\\delta}{\\delta w} \\big(h(x^{(i)}) - y^{(i)}\\big) \\\\\n",
    "& = \\frac {1}{m} \\cdot \\sum_{i=0}^{m} \\big(h(x^{(i)}) - y^{(i)}\\big) \\cdot \\frac{\\delta}{\\delta w} \\big(x^{(i)}\\times w - y^{(i)}\\big) \\\\\n",
    "& = \\frac {1}{m} \\cdot \\sum_{i=0}^{m} \\big(h(x^{(i)}) - y^{(i)}\\big) \\cdot x^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 결과를 그대로 weight에 업데이트 해줍니다. (여기서 lambda($\\lambda$)는 코드에서 learning rate의 역할을 합니다)\n",
    "\n",
    "$$\n",
    "w = w -  \\lambda \\frac {\\delta  L(x)} {\\delta w}\n",
    "$$\n",
    "\n",
    "위 공식을 코드화 한 것이 다음의 부분이라고 보시면 됩니다.\n",
    "\n",
    "```python\n",
    "w = w - learning_rate * ((y_predict - y) * x).mean()\n",
    "```\n",
    "\n",
    "Gradient Descent의 전체 코드는 다음과 같습니다.\n",
    "\n",
    "```python\n",
    "num_epoch = 100\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 0에서 1사이의 랜덤한 w를 생성한다.\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "# 100번의 loop를 돈다\n",
    "for epoch in range(num_epoch):\n",
    "    # h(x)를 연산한다.\n",
    "    y_predict = w * x\n",
    "\n",
    "    # h(x)와 y, 그리고 x를 활용하여 Loss Function의 편미분한 결과를 연산한다.\n",
    "    dw = ((y_predict - y) * x).mean()\n",
    "\n",
    "    # 편미분 결과에 learning rate를 곱한 후 weight에 그대로 업데이트 한다.\n",
    "    w = w - learning_rate * dw\n",
    "    \n",
    "# 결과를 출력하면, gradient descent가 언제나 우리가 원하는 w를 찾는다는 것을 확인할 수 있다.\n",
    "print(f\"{epoch:2} w = {w:.6f}, error = {error:.6f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
